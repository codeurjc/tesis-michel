% REPLICATION RESULTS

\subsection{Data for replication}

The original paper comes with a reproduction package, which we have found tremendously useful. It includes the complete list of commit hashes for all the analyzed repositories, and counts per repository of the main results. This allowed us to compute counts for our reduced set of 79 repositories. This way, we could do a fair comparison between the replication and the original results.

After computing our results for the 79 repositories in the original study, we found a discrepancy in the total number of commits. According to the original paper, 219,395 commits were analyzed. But computing from the list of commit hashes, we counted 174,505. To be consistent with other data in our study, which was extracted from that reproduction package, we used the second number in our tables. This fact does not impact the results, since we focus on the analysis of the reduced set of 79 repositories.

The scripts used for the original study are not available in its reproduction package. This is not a problem in itself, since we created our own software. But we could not determine the reason for some discrepancies, e.g., on the number of snapshots with build configuration (see details below). Maybe the heuristics coded into the original software took into account something that we missed, or maybe it didn't detect that configuration information in some cases.

Detailed logs of the execution are also not available in the reproduction package of the original study, making it difficult to explain some differences found in the compilability of a large number of commits in three repositories (see details below): we do not know if there was some error in the execution of the original study, or if we are missing something in our own.

\vspace{0.3cm}
\fbox{\begin{minipage}{0.9\textwidth}
\textbf{\textbf{RQ\textsubscript{1.1d}}: ``Is the data in the reproduction package of the original study enough for a replication?''} There is enough data for comparing results, even at the repository level. However, there is not enough data to exactly reproduce the original study: a copy of the git repositories, as they were when the analysis was performed, is missing; some relevant software is also missing (such as the one to detect the Maven configuration in a snapshot); and the logs of their execution for all snapshots are also missing, which makes it difficult to understand the reasons of some discrepancies when replicating the study.
\end{minipage}}

\newpage

\subsection{Compilability}

To answer RQ\textsubscript{1.1a} and RQ\textsubscript{1.1c} in detail, we follow the same process as the original study, classifying repositories in three categories according to their number of commits: short history (number of commits in the first quartile, Q1), medium history (Q2 and Q3) and long history (Q4). Repositories classified as short (20) have less than 203 commits, medium (39 repositories) have between 203 and 1,148 commits, and long repositories (20) have more than 1,148 commits. Then, we analyze the compilability of those snapshots for which we found configuration for Maven, assuming the rest could not be built because developers were not using automatic tools for building.

Tables~\ref{table:replication-compilability} and \ref{table:replication-compilability-2} show the results for compilability in the original study, and of our replication study. For the original study we computed, thanks to the details in its reproduction package, results for the reduced list of 79 repositories that we have analyzed. However, the results for the pristine study (with all its 100 repositories) are very similar to those of the reduced one. For example, the average compilability of the reduced study is 37.19, while for the pristine study it is 38.13, according to the original paper. For completeness, we run Wilcoxon's test on the compilability results of all projects of the previous and our replication studies, obtaining a p-value of 1.9077e-6. This confirms statistical significant differences between both cases.

% TABLE 1
\begin{table*}[h]
\caption{Compilable snapshots - Distribution (top: Reduced Original Study, bottom: Replication Study).}
%\label{table:originalResultsCompilability}
\label{table:replication-compilability}
\begin{center}
\resizebox{\textwidth}{!}{%
  \begin{tabular}{llllllll}
  \toprule
    \bf{Projects} & \multicolumn{7}{l}{Fraction built (in \%)} \\
                  & \multicolumn{7}{l}{} \\
                  & \bf{Min} & \bf{1st Qu.} & \bf{Median} & \bf{Mean} & \bf{3rd Qu.} & \bf{Max} & \bf{SD} \\  
  \midrule
  Short history  & 0.00     & 23.67        & 48.64       & 49.85      & 82.73       &  97.01   & 35.08 \\ 
  Medium history & 0.00     &  4.99        & 36.24       & 38.43      & 68.42       & 100.00   & 34.03 \\
  Long history   & 0.00     &  0.00        & 16.48       & 22.14      & 25.67       & 100.00   & 29.33 \\
  \midrule
  All            & 0.00     & 2.91         & 28.64       & \bf{37.19} & 66.00       & 100.00   & 34.25 \\ 
  \bottomrule
  \end{tabular}
}
\end{center}

\begin{center}
\resizebox{\textwidth}{!}{%
  \begin{tabular}{llllllll}
    \toprule
    \bf{Projects} & \multicolumn{7}{l}{Fraction built (in \%)} \\
                  & \multicolumn{7}{l}{} \\
                  & \bf{Min} & \bf{1st Qu.} & \bf{Median} & \bf{Mean} & \bf{3rd Qu.} & \bf{Max} & \bf{SD} \\  
  \midrule
  Short history  & 0.00     & 0.55         & 31.49       & 40.62      & 82.72        & 99.01     & 39.97 \\ 
  Medium history & 0.00     & 0.00         & 1.70        & 20.99      & 38.24        & 95.23     & 29.52 \\ 
  Long history   & 0.00     & 0.00         & 9.44        & 17.56      & 19.17        & 100.00    & 28.31 \\ 
  \midrule
  All            & 0.00     & 0.00         & 8.74        & \bf{25.1}  & 41.64        & 100.00    & 33.07 \\ 
  \bottomrule
  \end{tabular}
}
\end{center}
\end{table*}


%% TABLE 2
\begin{table*}[ht!]
  \caption{Compilable snapshots - Totals (top: Reduced Original Study, bottom: Replication Study).}
  %\label{table:originalResultsCompilability}
  \label{table:replication-compilability-2}
  \begin{center}
  
  \begin{tabular*}{\textwidth}{l|llllllll|lll}
  \toprule
    \bf{Projects} & \multicolumn{3}{c}{Total commits} \\
                  & \multicolumn{1}{c}{\bf{with build conf.}} & \multicolumn{1}{c}{\bf{Build success}} & \multicolumn{1}{c}{\bf{Fraction built}} \\
  \midrule
  Short history  &   2,311 &  1,129 & 48.85\% \\ 
  Medium history &  26,903 &  9,602 & 35.69\% \\
  Long history   &  72,597 & 12,006 & 16.54\% \\
  \midrule
  All            & 101,811 & 22,737 & 22.33\% \\ 
  \bottomrule

  \midrule
  Short history  &   2,311 &    876 & 37.91\%\\ 
  Medium history &  26,903 &  6,049 & 22.48\%\\ 
  Long history   &  72,597 &  7,739 & 10.66\%\\ 
  \midrule
  All            & 101,811 & 14,664 & 14.40\%\\ 
  \bottomrule
  \end{tabular*}
  \end{center}
\end{table*}



\vspace{0.2cm}
\fbox{\begin{minipage}{0.9\textwidth}
\textbf{ \textbf{RQ\textsubscript{1.1a}}: ``How many snapshots from the change history are compilable?''} 8.74\% or less of the commits were compilable for half of the analyzed projects. The result for the original study was 28.64\%.
Thus, results are very different, likely affected by the time passed (as we will discuss when answering RQ\textsubscript{1.1c}).
\end{minipage}}

\fbox{\begin{minipage}{0.9\textwidth}
\textbf{\textbf{RQ\textsubscript{1.1c}}: ``Has compilability degraded since the original study?''}
A much smaller fraction of the commits with build configuration can be built (22.33\% in the original study versus 14.66\% in our replication). When analyzing repositories by size, medium projects are more affected by this decrease in compilability, with half of them having a compilability of 1.70\% or less, while in the original study that number was 36.24\%. Short projects are also affected, and the less affected are long projects, but still they move from a median of 16.48\% to 9.44\% in our replication.
\end{minipage}}
\vspace{0.2cm}

We found significant differences in three repositories when checking for the existence of Maven configuration files (for all the others, results are exactly the same). In those three repositories we found 6,583 extra commits with a Maven configuration. When trying to build them, we could build 442 extra snapshots, which would increase the number of compilable snapshots to 15,106 (from a total of 108,394), or 13.93\% compilability, slightly less than 14,40\, as shown in Table~\ref{table:replication-compilability-2}.

We also checked all snapshots for configuration files of Ant and Gradle (we only found for Ant), finding 2,586 additional snapshots that could be built, leading to a total of 17,692 commits built. However, these results are not comparable with the original study, since they did not consider Ant.

Table~\ref{table:replication-buidability-summary} summarizes the results of all replications, with the different cases.
It should be noted that Table~\ref{table:replication-compilability} extends the information of the results reported in the ``Replication'' row of Table~\ref{table:replication-buidability-summary}, as it is the most comparable scenario between the original and the replication study.

\begin{table*}[ht!]
\caption{Original and replication studies (Summary of results). Replication\textsubscript{a}: use same build configs, Replication\textsubscript{b}: use all detected Maven config files, Replication\textsubscript{c}: use all detected Maven and Ant config files}
\label{table:replication-buidability-summary}
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lrrrrrr}
  \toprule
  & Repos.                        & All     & Commits           & Commits & Built & Built \\
  &                              & Commits & w/build conf    & built   & total          & mean of\\
  &                              &         &                   &         &                & projects \\
  \midrule 
  Pristine Original               & 100     & 174,505 & 132,484 & 31,696  & 23.92\%        & 37.74\% \\
  Reduced Original                &  79     & 139,389 & 101,811 & 22,737  & 22.33\%        & 37.19\% \\
  Replication\textsubscript{a}    &  79     & 139,389 & 101,811 & 14,664  & 14.40\%        & 25.09\% \\
  Replication\textsubscript{b}    &  79     & 139,389 & 108,394 & 15,106  & 13.93\%        & 25.42\% \\
  Replication\textsubscript{c}    &  79     & 139,389 & 117,124 & 17,692  & 15.10\%        & 24.85\% \\  
  \bottomrule
  \end{tabular}
}
\end{table*}


\subsection{Failure analysis}

To answer RQ\textsubscript{1.1b}, and provide some insight on the reasons for the answer to RQ\textsubscript{1.1c}, we analyzed build failures following the categorization of the original study. Table~\ref{table:allCauses} reports the classification results for the previous study and for our replication, in both cases for the 79 repositories of the reduced study.

\begin{table}[ht!]
\caption{Causes of failed builds.}
\label{table:allCauses}
\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}rrrrr}
\toprule
\multicolumn{1}{l}{}	    & \multicolumn{2}{c}{\bf{Original study}}    & \multicolumn{2}{c}{\bf{Replication study}}  \\
\bf{Categories}                 & \multicolumn{1}{r}{\#} & \multicolumn{1}{r}{\%} & \multicolumn{1}{r}{\#} & \multicolumn{1}{r}{\%}  \\
\midrule
Resolution  & 47,832 &  60.49 & 55,299 & 63.45  \\
Parsing     &  9,595 &  12.13 & 10,350 & 11.88  \\
Compilation &  2,807 &   3.55 &  2,544 &  2.92  \\
Other       & 18,839 &  23.83 & 18,954 & 21.75  \\
\midrule
All errors & 79,073 & 100.00 & 87,147 & 100.00 \\
\bottomrule
\end{tabular*}
\end{center}
\end{table}

Table~\ref{table:causesDetailed} shows the changes we observed when building snapshots with respect to the original study, according to the classification provided in Section~\ref{sec:buildability:metodology}. We could build 14,480 (63.68\%) of the successful builds in the original study (\textit{stable builds}). We could also build some snapshots that could not be built in the original study. In general, these correspond to external artifacts that were not available when they run their experiments, but were available when we run ours. These \emph{new build} snapshots amount to 184 (0.13\%).

% jgb: Keeping this old table, with only errors, in case we need to
% revert to it because of size limits (and write the data about new and
% stable builds somewhere else
%
% \begin{table}[]
% \caption{Changes in causes of failed builds}
% \label{table:causesDetailed}
% \begin{tabular}{rrrrrrr}
% 	\toprule
% \multicolumn{1}{l}{\bf{Categories}} & \multicolumn{2}{c}{\bf{Same error}} & \multicolumn{2}{c}{\bf{Different error}} & \multicolumn{2}{c}{\bf{New error}} \\
% \bf{(Replication)} & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\%} & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\%} & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\%} \\
% \midrule
% Resolution & 42,826  & 49.14  & 4,704 & 5.39  & 4,868  & 5.59                    \\
% Parsing                & 9,485                   & 10.88                   & 694                     & 0.79                    & 171                     & 0.20                    \\
% Compilation            & 2,544                   &  2.91                   & 0                       & 0.00                    & 0                       & 0.00                    \\
% Other                  & 12,443                  & 14.27                   & 6,194                   & 7.10                    & 3,218                   & 3.69                    \\
% \midrule
% All errors & 67,298 & 77.22 & 11,592 & 13.30 & 8,257 & 9.48 \\                 
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[]
\caption{Changes in builds (original to replication).}
\label{table:causesDetailed}
\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{l|rrrrrr|rr}
    \toprule
    & \multicolumn{6}{c|}{\bf{Errors}} & \multicolumn{2}{c}{\bf{Builds}} \\ 
    \multicolumn{1}{l|}{\bf{Categories}} & \multicolumn{2}{c}{\bf{Same}} & \multicolumn{2}{c}{\bf{Different}} & \multicolumn{2}{c|}{\bf{ New}} & \bf{Stable} & \bf{New} \\
  & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\%} & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\%} & \multicolumn{1}{c}{\#} & \multicolumn{1}{c|}{\%} & \# & \# \\
  \midrule
  Resolution             & 42,826                  & 49.14                   & 7,605                   & 5.39                    & 4,868                   & 5.59                    \\
  Parsing                & 9,485                   & 10.88                   & 694                     & 0.79                    & 171                     & 0.20                    \\
  Compilation            & 2,544                   &  2.91                   & 0                       & 0.00                    & 0                       & 0.00                    \\
  Other                  & 12,443                  & 14.27                   & 3,293                   & 7.10                    & 3,218                   & 3.69                    \\
  \midrule
  All  & 67,298 & 77.22 & 11,592 & 13.30 & 8,257 & 9.48 & 14,480 & 184 \\ 
  \bottomrule
  \end{tabular}
}
\end{table}

For 67,298 of the snapshots that failed to build, we reported the same exception as in the previous experiment (\textit{same error}). Therefore, adding these to \textit{stable build}, 81.778 (80.66\% of the snapshots with build configuration) behaved exactly as in the original study. On the other hand, for 13.30\% of the snapshots that failed for us, we found a different error than in the original study (\textit{different error}), while 9.48\% of the snapshots that failed for us did not fail in the original study (\textit{new error}).

Both in \textit{different error} and in \textit{new error}, one of the main causes for failed builds is \textit{Resolution}, and specifically, the resolution of external artifacts needed for the build. We can affirm that one of the main reasons for the degradation in compilability with time is the impossibility of recovering more and more of the dependencies of these projects as time passes. On the other hand, \textit{Compilation} errors only appear in \textit{same error}, since they only depend on using the same compiler (so if they failed in the replication, they should have failed in the original study). Comparing the number of \textit{Compilation} errors in the replication and in the original study, it decreased from 2,807 to 2,544. Some inspection raised the cause: new exceptions mask previous building errors.

\newpage
% jgb: SIZE We could remove the next para if needed
We studied in some detail the \textit{Resolution} category. One of the most recurrent cause was experiencing timeouts when accessing some external artifact, usually a dependency (5.12\% of total errors). This type of error appears only as \textit{different error} or \textit{new error}. Given that no long network failures were observed during the execution of our study, this is due to access to artifacts that are no longer available, which is consistent with the previous comment on dependencies.

\vspace{0.2cm}
\fbox{\begin{minipage}{0.9\textwidth}
\textbf{\textbf{RQ\textsubscript{1.1b}}: ``What types of errors prevent snapshots from being built?''}
The proportion of the different kinds of errors is similar to the original study. Most of the errors (63.45\%) are caused by resolution of dependencies and other external artifacts (60.49\% for the original study), these being the errors that contributed the most to the degradation of compilability. Parsing errors convey 11.88\% of the failures we detected, close to the 12.13\% reported in the original study. The same can be said for Compilation errors (2.92\% in our replication \emph{vs.} 3.55\% in the original study). About 80\% of the snapshots behaved exactly like in the previous study (they built, or they failed with the same error).
\end{minipage}}
